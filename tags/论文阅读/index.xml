<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>论文阅读 on 陈皮的博客</title>
    <link>https://y-m-m.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
    <description>Recent content in 论文阅读 on 陈皮的博客</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 13 May 2025 15:04:02 +0800</lastBuildDate>
    <atom:link href="https://y-m-m.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Pregel: A System for Large-Scale Graph Processing</title>
      <link>https://y-m-m.github.io/post/pregel/</link>
      <pubDate>Tue, 13 May 2025 00:00:00 +0000</pubDate>
      <guid>https://y-m-m.github.io/post/pregel/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;程序被表示为一个迭代序列，每一次迭代中的顶点可以接收上一次迭代中发送的消息，向其他顶点和输出边发送消息，或改变图的拓扑&lt;/li&gt;&#xA;&lt;li&gt;这种以顶点为中心的算法足够灵活，可以表达广泛的算法集&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对大图的高效处理是具有挑战性的。图算法通常表现出较差的内存访问局部性，每个顶点的工作很少，并且在执行的过程中并行度是变化的&lt;/li&gt;&#xA;&lt;li&gt;在许多机器上的Districution加剧了局部性问题，并增加了机器在计算过程中失败的概率&lt;/li&gt;&#xA;&lt;li&gt;在大规模分布式环境中，没有可扩展的通用系统来实现任意图表示上的任意图算法&lt;/li&gt;&#xA;&lt;li&gt;实现一个算法来处理一个大图通常意味着在下面的选项中进行选择&#xA;&lt;ul&gt;&#xA;&lt;li&gt;构建自定义的分布式基础架构，通常需要实现大量的工作，每个新的算法和图表示应用时都必须重复&lt;/li&gt;&#xA;&lt;li&gt;依靠现有的分布式计算平台，往往不适用于图处理&#xA;&lt;ul&gt;&#xA;&lt;li&gt;例如：MapReduce，非常适用于各种大规模计算问题&lt;/li&gt;&#xA;&lt;li&gt;它有时会被用来挖掘大图，但这会导致次优的性能和可用性问题&lt;/li&gt;&#xA;&lt;li&gt;处理数据的基本模型已经扩展，以方便聚合和类似SQL的查询，但这些扩展对于通常需要更好的拟合消息传递模型的图算法来说通常并不理想&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;使用单机图算法库，如BGL, LEDA, NetworkX, JDSL, Stanford GraphBase, 或者 FGL，限制可以处理的问题的规模&lt;/li&gt;&#xA;&lt;li&gt;使用现有的并行图系统&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Parallel BGL和CGMgraph库解决了并行图算法，但没有解决容错等对超大规模分布式系统很重要的问题&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;我们构建了一个可扩展的容错平台，具有足够灵活的API来表达任意的图算法&lt;/li&gt;&#xA;&lt;li&gt;Pregel程序的高层组织受到Valiant’s Bulk Synchronous Parallel model的启发&#xA;&lt;ul&gt;&#xA;&lt;li&gt;计算由一系列的迭代组成，称为超步&lt;/li&gt;&#xA;&lt;li&gt;在一个超步中，框架为每个顶点调用一个用户定义的函数，该函数规定了单个顶点V和单个超步S的行为&lt;/li&gt;&#xA;&lt;li&gt;它可以读取S-1中发送给V的消息，将消息发送给超步S+1中接收到的其他顶点，并修改状态V及其出边的状态&lt;/li&gt;&#xA;&lt;li&gt;消息通常沿着向外的边发送，但消息可以发送到任何标识符已知的顶点&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;以顶点为中心的方法类似于MapReduce&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用户专注于一个局部动作，独立地处理每个项&lt;/li&gt;&#xA;&lt;li&gt;系统将这些动作组合起来，从而将计算提升到一个大数据集&lt;/li&gt;&#xA;&lt;li&gt;通过设计，该模型非常适合分布式实现：它不公开任何检测超步执行顺序的机制，所有通信从超步S到超步S + 1&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;该模型的同步性使得在实现算法时更容易对程序语义进行推理，并确保Pregel程序在本质上不存在异步系统中常见的死锁和数据竞争&#xA;&lt;ul&gt;&#xA;&lt;li&gt;由于典型的图计算比机器拥有更多的顶点，因此需要能够平衡机器的负载，使得超步之间的同步不会增加过多的延迟&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;model-of-computation计算模型&#34;&gt;MODEL OF COMPUTATION(计算模型)&lt;/h1&gt;</description>
    </item>
    <item>
      <title>MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs</title>
      <link>https://y-m-m.github.io/post/mllms%E7%BB%BC%E8%BF%B0evaluation/</link>
      <pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://y-m-m.github.io/post/mllms%E7%BB%BC%E8%BF%B0evaluation/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;根据评估能力划分的汇总基准类型&lt;/li&gt;&#xA;&lt;li&gt;基础能力&lt;/li&gt;&#xA;&lt;li&gt;模型自分析&lt;/li&gt;&#xA;&lt;li&gt;扩展应用&lt;/li&gt;&#xA;&lt;li&gt;基准建设的典型流程&lt;/li&gt;&#xA;&lt;li&gt;数据收集&lt;/li&gt;&#xA;&lt;li&gt;数据标注&lt;/li&gt;&#xA;&lt;li&gt;注意事项&lt;/li&gt;&#xA;&lt;li&gt;由判断、度量和工具组成的系统评价方式&lt;/li&gt;&#xA;&lt;li&gt;对下一个benchmark的展望&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1 Introduction&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2014.27.43.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对MLLM evaluation进行全面而系统的综述&#xA;&lt;ul&gt;&#xA;&lt;li&gt;评估了哪些能力？&#xA;&lt;ul&gt;&#xA;&lt;li&gt;分层分类&lt;/li&gt;&#xA;&lt;li&gt;在top level，benchmarks可以被分类为&#xA;&lt;ul&gt;&#xA;&lt;li&gt;evaluations of foundational capabilities&lt;/li&gt;&#xA;&lt;li&gt;Model behavior&lt;/li&gt;&#xA;&lt;li&gt;extended applications&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;如何构建benchmark?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;benchmark构建的方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;样本的收集&lt;/li&gt;&#xA;&lt;li&gt;QA pairs的描述&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;评估模型时需要特别注意的问题&#xA;&lt;ul&gt;&#xA;&lt;li&gt;数据污染&lt;/li&gt;&#xA;&lt;li&gt;基准多样性&lt;/li&gt;&#xA;&lt;li&gt;样本量&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;如何衡量performance?&#xA;&lt;ul&gt;&#xA;&lt;li&gt;三种代表性的方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基于人的评价&lt;/li&gt;&#xA;&lt;li&gt;基于LLM/MLLM的评价&lt;/li&gt;&#xA;&lt;li&gt;基于脚本的评价&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;两种主要评价指标以及四种评价工具包&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;New benchmark的方向在哪里？&#xA;&lt;ul&gt;&#xA;&lt;li&gt;以能力为导向的评价&lt;/li&gt;&#xA;&lt;li&gt;以任务为导向的评价&lt;/li&gt;&#xA;&lt;li&gt;融入更多的模态&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;2-background&#34;&gt;2 Background&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;简单的讨论了MLLMs的关键，包括architecture和训练&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;21-architecture-of-mllm&#34;&gt;2.1 Architecture of MLLM&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;典型的MLLM包括三个模块，a modality encoder, a LLM and a connector between them&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2014.28.41.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;MLLM的核心是统一的自回归建模&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2014.29.11.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;22-training-of-mllm&#34;&gt;2.2 Training of MLLM&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2014.29.46.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</title>
      <link>https://y-m-m.github.io/post/vita-1.5/</link>
      <pubDate>Fri, 28 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://y-m-m.github.io/post/vita-1.5/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在本文中，我们提出了一种精心设计的多阶段训练方法，通过逐步训练LLMs来理解视觉和语言信息，最终实现流利的视觉和语音交互；&lt;/li&gt;&#xA;&lt;li&gt;保留了强大的version-language能力，在没有单独的ASR和TTS模块的情况下实现了高效的语音对话能力，显著加快了多模态端到端的响应速度；&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ASR 自动语音识别，将语音转换为文本&lt;/li&gt;&#xA;&lt;li&gt;TTS 文本到语音合成，将文本转换为语音&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1 Introduction&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MLLMs中视觉和语音的整合并不简单，因为它们存在固有差异，常常导致训练过程的冲突&#xA;&lt;ul&gt;&#xA;&lt;li&gt;图像等视觉数据传递的是空间信息&lt;/li&gt;&#xA;&lt;li&gt;语音数据传递的是时间序列的动态变化&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;传统的语音到语音系统依赖于单独的模块ASR和TTS，这会增加延迟、降低一致性，限制了它们在实际应用中的一致性&lt;/li&gt;&#xA;&lt;li&gt;VITA-1.5&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一个multimodal LLM&lt;/li&gt;&#xA;&lt;li&gt;通过精心设计的三阶段训练方法整合了视觉、语言和语音&lt;/li&gt;&#xA;&lt;li&gt;这种策略逐步整合视觉和语音数据，在缓解模态冲突的同时保持了较强的多模态性能&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Stages&#xA;&lt;ul&gt;&#xA;&lt;li&gt;第一阶段，我们训练visual adapters，用描述性caption和视觉QA数据，关注version-language&#xA;&lt;ul&gt;&#xA;&lt;li&gt;这一步建立了模型的基本视觉能力，允许robust图像和视频理解&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;第二阶段，使用speech-transcription paired数据，训练一个audio encoder，再使用speech QA数据进行微调，引入audio input processing&#xA;&lt;ul&gt;&#xA;&lt;li&gt;这一步使模型具有有效的理解和响应音频输入的能力&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;第三阶段，训练一个audio decoder，使其能够实现端到端的语音输出，省去了外部的TTS模块&#xA;&lt;ul&gt;&#xA;&lt;li&gt;使VITA-1.5能够生成流畅的语音回复，促进了多模态对话系统的自然性和交互性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;在与图像、视频和语音理解相关的各种基准进行了广泛的评估，将结果与open-source和proprietary模型进行比较&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VITA-1.5显示出与基于图像/视频的MLLMs相当的感知和推理能力，并在语音能力方面取得显著进步&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;2-related-work&#34;&gt;2 Related work&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;近年来，得益于GPTs, LLaMA, Alpaca, Vicuna和Mistral等大语言模型的快速发展，研究者们通过多模态对齐和指令调优，成功的将文本理解扩展到多模态理解/推理&lt;/li&gt;&#xA;&lt;li&gt;例如&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LLaVA, Qwen-VL, Cambrain-1, Mini-Gemini, MiniGPM-V 2.5, DeepSeek-VL和SLiME在图像预测和推理方面取得了重大进展&lt;/li&gt;&#xA;&lt;li&gt;LongVA, Video-LLaVA等模型展示出了视频理解的最新进展&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;很少有开源模型涉及多模态交互能力&lt;/li&gt;&#xA;&lt;li&gt;VITA-1.0对引入语音进行人机交互进行了初步尝试，但引入额外的语音数据对模型原有的多模态能力来说是个挑战&#xA;&lt;ul&gt;&#xA;&lt;li&gt;语音生成通常依赖于现有的TTS系统，导致高延迟&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;VITA-1.5&#xA;&lt;ul&gt;&#xA;&lt;li&gt;采用了精致的训练策略，在感知四个模态的数据(视频、图像、文本、音频)方面表现优异&lt;/li&gt;&#xA;&lt;li&gt;实现了接近实时的视觉和语音交互&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;3-vita-15&#34;&gt;3 VITA-1.5&lt;/h1&gt;&#xA;&lt;h2 id=&#34;31-model-atchitecture&#34;&gt;3.1 Model Atchitecture&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2014.06.41.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;输入&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Multimodal Encoder-Adaptor-LLM的配置&lt;/li&gt;&#xA;&lt;li&gt;将视觉/听觉 transformer 和 多层connector 联合 大语言模型一起训练，旨在增强对视觉、音频和语言的统一认识&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;输出&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VITA-1.5有自己的端到端语音模块，而不是像VITA-1.0那样使用外部模块&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;311-visional-modality&#34;&gt;3.1.1 Visional Modality&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VIsional Encoder&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Intern ViT-300M作为视觉编码器，输入图像大小为448*448个像素&lt;/li&gt;&#xA;&lt;li&gt;每个图像生成256个visual tokens&lt;/li&gt;&#xA;&lt;li&gt;对于高分辨率的图像，VITA-1.5采用动态补丁策略捕捉局部细节，提高图像理解的准确性&#xA;&lt;ul&gt;&#xA;&lt;li&gt;动态补丁策略是一种在图像处理或计算机视觉任务中用于捕捉局部细节的技术，通过动态调整patch的大小或位置，来更好的适应图像中的不同区域和细节&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Video Processing&#xA;&lt;ul&gt;&#xA;&lt;li&gt;视频被理解为一种特殊类型的多图像输入&lt;/li&gt;&#xA;&lt;li&gt;小于4s：均匀采样4帧&lt;/li&gt;&#xA;&lt;li&gt;4~16s：每秒采样1帧&lt;/li&gt;&#xA;&lt;li&gt;大于16s：均匀采样16帧&lt;/li&gt;&#xA;&lt;li&gt;不对视频帧运用动态补丁，以避免过多的视觉令牌阻碍处理效率&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Vision Adapter&#xA;&lt;ul&gt;&#xA;&lt;li&gt;使用两层MLP(多层感知机)来将视觉特征映射到适合后续LLM理解的visual tokens&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;312-audio-modality&#34;&gt;3.1.2 Audio Modality&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Speech Encoder&#xA;&lt;ul&gt;&#xA;&lt;li&gt;我们的audio encoder模块由多个下采样卷积层(4倍下采样)和24个Transformer块(隐藏尺寸为1024)组成&#xA;&lt;ul&gt;&#xA;&lt;li&gt;下采样层有助于降低音频特征的帧率，提高LLM的处理速度&lt;/li&gt;&#xA;&lt;li&gt;下采样是通过降低采样率来减少数据量的过程，下采样会减少信号的帧率，同时保留关键信息，从而提高后续处理的效率&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;参数为350M，输出帧率为12.5Hz&lt;/li&gt;&#xA;&lt;li&gt;Mel-filter bank features作为audio encoder的输入，窗口大小为25ms，步长 10ms&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Speech Adapter&#xA;&lt;ul&gt;&#xA;&lt;li&gt;由多个具有两倍下采样的卷积层组成&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Speech Decoder&#xA;&lt;ul&gt;&#xA;&lt;li&gt;TiCodec(编解码器)&lt;/li&gt;&#xA;&lt;li&gt;制定了一个大小为1024的单码本，简化了推理阶段的解码过程&lt;/li&gt;&#xA;&lt;li&gt;负责将连续的语音信号编码为40Hz的离散的speech tokens，同时能够将speech tokens解码为采样率24000Hz的语音信号&lt;/li&gt;&#xA;&lt;li&gt;两种解码器&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Non-Autoregressive (NAR) Speech Decoder 对文本标记进行全局处理，并对语义特征进行建模，目的是生成speech tokens的初始分布&lt;/li&gt;&#xA;&lt;li&gt;Autoregressive (AR) Speech Decoder 基于NAR产生的语音信息，逐步产生更高质量的speech tokens，再使用codec model将最后的speech tokens解码为连续的语音信号流&lt;/li&gt;&#xA;&lt;li&gt;NAR和AR均采用4个LLaMA解码层，hidden size为896，参数大小为大约120M&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;32-training-data&#34;&gt;3.2 Training Data&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2014.07.50.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>MIST : Multi-modal Iterative Spatial-Temporal Transformer(多模态迭代时空转换器) for Long-form Video Question Answering</title>
      <link>https://y-m-m.github.io/post/mist/</link>
      <pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://y-m-m.github.io/post/mist/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;2022-12-19 arXiv&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;多模态VQA模型: 短视频片段/图片 -&amp;gt; 长视频&#xA;&lt;ul&gt;&#xA;&lt;li&gt;使用密集视频采样策略计算太复杂&lt;/li&gt;&#xA;&lt;li&gt;稀疏采样难以应对多粒度、多事件可视化推理&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;MIST的特点&#xA;&lt;ul&gt;&#xA;&lt;li&gt;dense spatialtemporal self-attention-&amp;gt;级联的片段和区域选择模块，自适应地选择与问题本身密切相关的帧和图像区域&lt;/li&gt;&#xA;&lt;li&gt;然后通过attention model对不同粒度的视觉概念进行高效处理&lt;/li&gt;&#xA;&lt;li&gt;MIST在多个层次上迭代地进行选择和关注，以支持对多个事件的推理&lt;/li&gt;&#xA;&lt;li&gt;使用数据集&#xA;&lt;ul&gt;&#xA;&lt;li&gt;AGQA, NExT-QA, STAR, Env-QA(平均时长从12s到44s不等)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1 Introduction&lt;/h1&gt;&#xA;&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;多事件推理(复杂的时序推理，如多事件推理，因果关系)&lt;/li&gt;&#xA;&lt;li&gt;不同粒度的视觉概念之间的相互作用(可能涉及跨越不同事件的更复杂的物体、关系和事件的交互)&lt;/li&gt;&#xA;&lt;li&gt;目前的多模态transformer依赖于密集的自注意力，随视频时间的延长，计算成本呈指数增长 -&amp;gt; 只对每个视频采样3帧或4帧，不管其长度如何&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;新模型mist&#34;&gt;新模型MIST&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;考虑所有事件的一般内容，关注与问题相关的少数事件的细节&lt;/li&gt;&#xA;&lt;li&gt;MIST将稠密的联合时空自注意力机制分解为问题条件的级联片段和区域选择模块，以及多模态多粒度特征上的时空自注意力&#xA;&lt;ul&gt;&#xA;&lt;li&gt;级联选择通过关注与问题相关的片段和区域，降低了计算成本并提高了性能&lt;/li&gt;&#xA;&lt;li&gt;通过对片段和图像块的自注意力，更好的捕获了不同粒度的视觉概念之间的相互作用&lt;/li&gt;&#xA;&lt;li&gt;通过迭代的进行选择和自注意力，MIST可以在多个事件上进行推理，并更好的进行时间和因果推理&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;2-related-work&#34;&gt;2 Related work&lt;/h1&gt;&#xA;&lt;h2 id=&#34;video-question-answering&#34;&gt;Video question answering&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;长视频VQA任务主要关注于对情节和、社会交往的理解，和更复杂的时空推理能力；目前的工作在前两类基准上取得了可喜的成果，本文主要关注于最后一类问题&lt;/li&gt;&#xA;&lt;li&gt;方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;早期提出了各种基于LSTM和图神经网络的模型 来捕获 跨模态的或动作与外观的交互&lt;/li&gt;&#xA;&lt;li&gt;最近的一项工作将图建模集成到transformers中，以显式地捕获视频中的对象及其关系&lt;/li&gt;&#xA;&lt;li&gt;许多工作直接在下游videoQA任务上对预训练模型进行微调&lt;/li&gt;&#xA;&lt;li&gt;本文对自注意力机制进行了修正，以便更好的进行多时间、多粒度的视觉推理&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;将预训练的模型迁移到下游任务中&#34;&gt;将预训练的模型迁移到下游任务中&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;预训练的vision-language Transformers, 如CLIP -&amp;gt; 下游任务，如目标检测，图像生成和视频文本检索&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CLIP4Clip: 均值池化，Transformer等-&amp;gt;更好的表示视频&lt;/li&gt;&#xA;&lt;li&gt;CLIP2Video: 提出了时间差分块来更好的捕捉运动信息&lt;/li&gt;&#xA;&lt;li&gt;我们保留了预训练模型的优点，并改进了它在下游任务上的弱点&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;long-form-video-modeling&#34;&gt;Long-form video modeling&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;利用视频的自然特性，减少计算量&lt;/li&gt;&#xA;&lt;li&gt;本文考虑了QA任务的特点，以问题为导向，减少计算量，提高性能&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;iterative-attention&#34;&gt;Iterative Attention&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MIST利用了VQA中的多模态对应关系来引导迭代注意&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;3-method&#34;&gt;3 Method&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VQA任务目标&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2011.23.49.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;MIST回答问题的三步&#xA;&lt;ol&gt;&#xA;&lt;li&gt;利用预训练模型提取输入特征&lt;/li&gt;&#xA;&lt;li&gt;迭代地在选定的特征集合上perform自注意力，以执行多事件推理&lt;/li&gt;&#xA;&lt;li&gt;根据得到的video, question, answer features预测答案&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2011.24.30.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;31-input-representation&#34;&gt;3.1 Input Representation&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;视频部分&#xA;&lt;ul&gt;&#xA;&lt;li&gt;将视频分为K个长度均匀的片段，每个片段包含T帧，每帧被分为N个小块 -&amp;gt; 图片块和帧的 [CLS] token 以N和T计数&lt;/li&gt;&#xA;&lt;li&gt;vision-language Transformer，如CLIP，All-in-one with frozen parameters，提取所有帧的图像块级别的特征&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2013.54.55.png&#34; alt=&#34;&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;由于分段特征是单独提取的，表示其在整个视频中的时间位置Pt ∈ {φt(i)|i ∈ [0, K · T ]}和帧索引&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;文本部分&#xA;&lt;ul&gt;&#xA;&lt;li&gt;将问题表征为单词序列&lt;/li&gt;&#xA;&lt;li&gt;将它输入到vision-language Transformer来获取单词级别的特征&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2013.55.42.png&#34; alt=&#34;&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;w1 corresponds to [CLS] and w2, &amp;hellip;, wM are words in question&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;32-ista&#34;&gt;3.2 ISTA&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Iterative Spatial-Temporal Attention Layer 迭代时空注意力层&lt;/li&gt;&#xA;&lt;li&gt;迭代的选择基于问题的长视频中的片段和区域，然后对选择的片段和区域进行多事件推理&lt;/li&gt;&#xA;&lt;li&gt;步骤&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Segment selection&lt;/li&gt;&#xA;&lt;li&gt;Region selection&lt;/li&gt;&#xA;&lt;li&gt;时空自注意力&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2013.56.26.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;segment-selection&#34;&gt;Segment Selection&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;给定一组image patch特征x，计算segments和问题的特征 -&amp;gt; 选择Topk segments的patch features&lt;/li&gt;&#xA;&lt;li&gt;执行temporal attention&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对每一个第k segment第t frame做空间维度的特征池化&lt;/li&gt;&#xA;&lt;li&gt;将frame features沿时间维度进行池化得到segment features&lt;/li&gt;&#xA;&lt;li&gt;Question feature通过对word features进行池化得到&lt;/li&gt;&#xA;&lt;li&gt;池化函数可以根据使用的Transformer的具体类型，从mean pooling, first token pooling, simple MLP layer等中选择&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;region-selection&#34;&gt;Region Selection&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对采样所得的帧，选择与问题最相关的patches&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;spatial-temporal-self-attention&#34;&gt;Spatial-Temporal Self-Attention&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;给定选定的frames和选定的regions，使用一个self-attention layer推理出一个融合的特征向量来联合表示question和video&lt;/li&gt;&#xA;&lt;li&gt;只保留选择出的patches&lt;/li&gt;&#xA;&lt;li&gt;嵌入类型特征 -&amp;gt; 多头注意力机制获得所有输入令牌的上下文特征&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;iterative-execution-of-ista&#34;&gt;Iterative Execution of ISTA&lt;/h3&gt;&#xA;&lt;h2 id=&#34;answer-prediction&#34;&gt;Answer prediction&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;将所有ISTA层的token特征池化&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2013.58.07.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;计算Xo和所有通过预训练模型获取的候选答案之间的相似度，选择相似度最大的答案作为最终预测答案&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;4-experiments&#34;&gt;4 Experiments&lt;/h1&gt;&#xA;&lt;h2 id=&#34;41-datasets&#34;&gt;4.1 Datasets&lt;/h2&gt;&#xA;&lt;h3 id=&#34;agqa&#34;&gt;AGQA&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一个开放的，用于合成时空推理的VQA基准，使用v2版本(有更平衡的分布)&lt;/li&gt;&#xA;&lt;li&gt;2.27M QA对，9.7K videos，平均长度30s&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;next-qa&#34;&gt;NExT-QA&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用于因果和时序推理的多选择视频问答基准&lt;/li&gt;&#xA;&lt;li&gt;包括5440个视频，平均长度44s，约25K个问题&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;star&#34;&gt;STAR&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用于情景推理的多选择VQA基准&lt;/li&gt;&#xA;&lt;li&gt;22个视频片段，平均长度12s，约60K个问题&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;env-qa&#34;&gt;Env-QA&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;开放的，用于动态环境理解的VQA基准&lt;/li&gt;&#xA;&lt;li&gt;23K个自我中心视频，平均长度20s，85K问题，是在虚拟环境AI2THOR上收集的&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;42-implementation-details&#34;&gt;4.2 Implementation Details&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;可以建立在大多数预训练的多模态Transformer上，在实验中尝试了针对图像语言预训练的模型CLIP(ViT-B/32)和针对视频语言预训练的模型All-in-One-Base&lt;/li&gt;&#xA;&lt;li&gt;Topk = 2, Topj = 12, the layer of ISTA L = 2, 每个视频采样32帧，并将其分割成K = 8个segments&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;43-comparison-with-state-of-the-arts&#34;&gt;4.3 Comparison with State-of-the-arts&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;与最先进的SOTA方法进行比较&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2013.59.15.png&#34; alt=&#34;&#34;&gt;&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2013.59.34.png&#34; alt=&#34;&#34;&gt;&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2013.59.55.png&#34; alt=&#34;&#34;&gt;&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2014.00.23.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;我们提出的方法达到了最先进的性能，并且在所有数据集上都优于现有的方法&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;44-comparison-with-baselines&#34;&gt;4.4 Comparison with Baselines&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在基于CLIP的MIST框架中，我们通过在AGQA v2数据集上微调相同的预训练输入表示，将ISTA与其他解决方案进行比较&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MeanPool: 简单的抽取帧特征的平均值作为整个视频的表示&lt;/li&gt;&#xA;&lt;li&gt;Trans.-Frame: 遵循CLIP4Clip中的seqTransf类型，利用Transformer对帧特征进行自注意力来表示视频&lt;/li&gt;&#xA;&lt;li&gt;Trans.-Patch: 类似Trans.-Frame, 但它在所有patch tokens上执行自注意力&lt;/li&gt;&#xA;&lt;li&gt;Divided STA: 我们遵循视频分类模型中的Transformer对image patches执行uni-modal two-step SpaceTime Attention&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2014.00.59.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;如上结果表示，ISTA优于其他变体&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;原因&lt;/li&gt;&#xA;&lt;li&gt;长格式视频的内容往往复杂多样，简单的聚合所有帧特征的方法，可能会造成信息丢失&lt;/li&gt;&#xA;&lt;li&gt;长格式视频问答需要更强大的时空推理能力来关注视频的一些细节，而均值池化只在捕获整体内容上表现良好&lt;/li&gt;&#xA;&lt;li&gt;长格式QA中考虑region information是有帮助的，但稠密的自注意力和divided STA都没有考虑多粒度之间的相互作用，性能提升有限&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;45-ablation-study&#34;&gt;4.5 Ablation Study&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;提出MIST的几组变体，显示其关键组件的有效性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;effect-of-each-component-in-ista&#34;&gt;Effect of each component in ISTA&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;删除ISTA层中的关键模块，即Segment Selection(SS), Region Selection(RS), Self-attention layer(STA)&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2014.01.50.png&#34; alt=&#34;&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;移除Segment Selection会引入大量的无关区域信息，hurt the performance&lt;/li&gt;&#xA;&lt;li&gt;Segment Selection对于多事件推理很重要，会损害需要时序推理的问题的表现&lt;/li&gt;&#xA;&lt;li&gt;去除STA时，性能下降显著，原因与mean pooling类似，我们需要一个强大的模型来捕捉多粒度推理&lt;/li&gt;&#xA;&lt;li&gt;去除RS无明显影响，原因是视频中的objects相对较少，经过temporal attention后，patch number大大减少；但使用spatial selection module可以显著降低计算成本&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;effects-of-different-ista-configurations&#34;&gt;Effects of different ISTA configurations&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2014.02.53.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Survey on Multimodal Large Language Models</title>
      <link>https://y-m-m.github.io/post/mllm%E7%BB%BC%E8%BF%B0/</link>
      <pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://y-m-m.github.io/post/mllm%E7%BB%BC%E8%BF%B0/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;2024-11-14 IEEE&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1 Introduction&lt;/h1&gt;&#xA;&lt;h2 id=&#34;mllm的两个代表性特征&#34;&gt;MLLM的两个代表性特征&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基于Billion-scale parameters LLM&lt;/li&gt;&#xA;&lt;li&gt;使用新的训练范式，例如多模态教学调优&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;mllm的扩展&#34;&gt;MLLM的扩展&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;更好的粒度支持，对用户提示进行更精细的控制；&lt;/li&gt;&#xA;&lt;li&gt;增强了对输入输出模态的支持，如图像、视频、音频和点云；NExT-GPT支持不同模态的输出；&lt;/li&gt;&#xA;&lt;li&gt;改进了语言支持&lt;/li&gt;&#xA;&lt;li&gt;扩展到更多领域和使用场景&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;mllm的时间线&#34;&gt;MLLM的时间线&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2010.31.51.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;surveys主要内容&#34;&gt;surveys主要内容&lt;/h2&gt;&#xA;&lt;h3 id=&#34;mllms重要方面的综述&#34;&gt;MLLMs重要方面的综述&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;主流架构&lt;/li&gt;&#xA;&lt;li&gt;训练策略和训练数据&lt;/li&gt;&#xA;&lt;li&gt;效果评价&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;mllms的重要问题&#34;&gt;MLLMs的重要问题&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;那些方面可以进一步完善和扩展&lt;/li&gt;&#xA;&lt;li&gt;如何缓解多模态幻觉问题&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;三种关键技术&#34;&gt;三种关键技术&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;M-ICL(多模态上下文学习) 在推理阶段常用的提高小样本性能的有效技术&lt;/li&gt;&#xA;&lt;li&gt;M-CoT(多模态思维链) 用于复杂的推理任务&lt;/li&gt;&#xA;&lt;li&gt;开发基于LLM的系统来解决复合推理任务或解决普通用户查询的一般思路&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;2-architecture&#34;&gt;2 Architecture&lt;/h1&gt;&#xA;&lt;h2 id=&#34;mllm的模块构成&#34;&gt;MLLM的模块构成&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;预训练的模态编码器&lt;/li&gt;&#xA;&lt;li&gt;预训练的LLM&lt;/li&gt;&#xA;&lt;li&gt;连接他们的接口&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;21-modality-encoder&#34;&gt;2.1 Modality Encoder&lt;/h2&gt;&#xA;&lt;h3 id=&#34;作用&#34;&gt;作用&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;将原始信息压缩成更紧凑的表示；&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;常用modality-encoder&#34;&gt;常用Modality Encoder&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;使用已经对齐到其他模态的预训练编码器，而无需从头开始训练；&#xA;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2010.33.28.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;MiniGPT-4 -&amp;gt; EVA-CLIP encoder&lt;/li&gt;&#xA;&lt;li&gt;Osprey -&amp;gt; ConvNext-L encoder&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;考虑的因素&#34;&gt;考虑的因素&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;分辨率&#xA;&lt;ul&gt;&#xA;&lt;li&gt;提高分辨率可以获得显著的性能增益&lt;/li&gt;&#xA;&lt;li&gt;提高分辨率方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;直接缩放法 直接将高分辨率图像输入到编码器中 (CogAgent采用双编码机制，两个编码器分别处理高、低编码器图像)&lt;/li&gt;&#xA;&lt;li&gt;分片法 将一幅高分辨率图像切割成块，复用低分辨率编码器 (Monkey和SPHINX将一幅大图像分割成更小的图像块，并将子图像和降采样后的高分辨率图像一起发送给Encoder)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Pingi使用CLAP模型作为音频编码器&lt;/li&gt;&#xA;&lt;li&gt;ImageBind-LLM 使用ImageBind编码器，支持编码图像、文本、音频、深度、温度和惯性测量单元( Inertial Measurement Unit，IMU )数据&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;参数大小&lt;/li&gt;&#xA;&lt;li&gt;训练语境&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;22-pre-trained-llm&#34;&gt;2.2 Pre-trained LLM&lt;/h2&gt;&#xA;&lt;h3 id=&#34;常用pre-trained-llm&#34;&gt;常用pre-trained LLM&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://y-m-m.github.io/images/%E6%88%AA%E5%B1%8F2025-03-05%2010.35.04.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
