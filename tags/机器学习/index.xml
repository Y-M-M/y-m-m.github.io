<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on 陈皮的博客</title>
    <link>https://y-m-m.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on 陈皮的博客</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 14 Mar 2025 09:34:54 +0800</lastBuildDate>
    <atom:link href="https://y-m-m.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>机器学习</title>
      <link>https://y-m-m.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://y-m-m.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</guid>
      <description>&lt;h1 id=&#34;1-分类与决策模型&#34;&gt;1. 分类与决策模型&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;学习任务分类&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;监督学习(supervised learning)&lt;/li&gt;&#xA;&lt;li&gt;无监督学习(unsupervised learning)&lt;/li&gt;&#xA;&lt;li&gt;半监督学习(semi-supervised learning)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一半的数据有标签&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;强化学习(reinforcement learning)&lt;/li&gt;&#xA;&lt;li&gt;弱监督学习(weak supervised learning)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;数据有标签，但标签和任务不相关&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;分类&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;分类是什么&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对于给定一个对象，从一个事先定好的分类体系中挑出一个(或者多个)最适合该对象的类别&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对象：可以是任何东西&lt;/li&gt;&#xA;&lt;li&gt;事先定好的分类体系：可能有结构&lt;/li&gt;&#xA;&lt;li&gt;最适合：判断标准&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;便于今后查找其最直接的映射&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;分类系统可以是层次结构&lt;/li&gt;&#xA;&lt;li&gt;分类模式&#xA;&lt;ul&gt;&#xA;&lt;li&gt;2类问题，属于或不属于&lt;/li&gt;&#xA;&lt;li&gt;多类问题，多个类别，可拆分成2类问题&lt;/li&gt;&#xA;&lt;li&gt;一个对象可以属于多类(multi-label)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;分类的基本步骤&#xA;&lt;ul&gt;&#xA;&lt;li&gt;定义分类体系&lt;/li&gt;&#xA;&lt;li&gt;将预先分类过的数据作为训练集&lt;/li&gt;&#xA;&lt;li&gt;从训练集中的处分类模型&lt;/li&gt;&#xA;&lt;li&gt;用分类模型对物体进行分类&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;分类问题的评价方式&#xA;&lt;ul&gt;&#xA;&lt;li&gt;准确率 (P, precision)  判断异常的正确率&lt;/li&gt;&#xA;&lt;li&gt;召回率 (R, recall)  从所有异常中找出异常的概率&lt;/li&gt;&#xA;&lt;li&gt;准确率与召回率很难兼得-&amp;gt;F-measure&#xA;&lt;ul&gt;&#xA;&lt;li&gt;F = 1 / (α * (1 / P) + (1 - α) * (1 / R) )&lt;/li&gt;&#xA;&lt;li&gt;(α = 0.5时) F&lt;!-- raw HTML omitted --&gt;1&lt;!-- raw HTML omitted --&gt; = 2PR / P + R&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;11-决策树基本算法&#34;&gt;1.1 决策树基本算法&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;二分类学习任务&#xA;&lt;ul&gt;&#xA;&lt;li&gt;属性&lt;/li&gt;&#xA;&lt;li&gt;属性值&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;决策树学习的目的&#xA;&lt;ul&gt;&#xA;&lt;li&gt;为了产生一颗泛化能力强的决策树，即处理未见示例能力强&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;基本算法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;决定分类属性&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;对目前的数据表，建立一个节点N&lt;/li&gt;&#xA;&lt;li&gt;如果数据库中的数据都属于同一个类，N就是叶节点，在叶节点上标出所属的类&lt;/li&gt;&#xA;&lt;li&gt;如果数据表中没有其他属性可考虑，则N也是叶节点，按照少数服从多数的原则在叶节点上标出所属类别&lt;/li&gt;&#xA;&lt;li&gt;否则，根据属性信息增益值选出一个最佳属性作为节点N的测试属性&lt;/li&gt;&#xA;&lt;li&gt;节点属性选定后，对于该属性中的每个值再生成一个分支&lt;/li&gt;&#xA;&lt;li&gt;-&amp;gt;递归&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;关键：选择最优划分属性&#xA;&lt;ul&gt;&#xA;&lt;li&gt;希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的“纯度”越来越高&lt;/li&gt;&#xA;&lt;li&gt;衡量“纯度”的指标&#xA;&lt;ul&gt;&#xA;&lt;li&gt;信息增益&#xA;&lt;ul&gt;&#xA;&lt;li&gt;信息熵(香农用“信息熵”的概念来描述信源的不确定性)&lt;/li&gt;&#xA;&lt;li&gt;信息增益 Gain(D, a) (D是信息熵，a是属性)&lt;/li&gt;&#xA;&lt;li&gt;问题：信息增益准则对可取值数目较多的属性有所偏好&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;增益率&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Gain_ratio(D, a) = Gain(D, a) / IV(a)&lt;/li&gt;&#xA;&lt;li&gt;IV(a)属性a的“固有值“，属性a的可能的取值数目越多，IV(a)的值通常会越大&lt;/li&gt;&#xA;&lt;li&gt;增益率准则对可取值数目较少的属性有所偏好&lt;/li&gt;&#xA;&lt;li&gt;著名的C4.5决策树算法综合了信息增益准则和信息率准则的特点，先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益最高的&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;基尼指数&#xA;&lt;ul&gt;&#xA;&lt;li&gt;著名的CART决策树算法&lt;/li&gt;&#xA;&lt;li&gt;Gini(D)越小，纯度越高&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;特征选择&#xA;&lt;ul&gt;&#xA;&lt;li&gt;目标：寻找最优特征子集；剔除不相关或冗余的特征，从而达到减少特征额书，提高模型精确度，减少运行时间的目的&lt;/li&gt;&#xA;&lt;li&gt;常见方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Pearson相关系数&lt;/li&gt;&#xA;&lt;li&gt;卡方验证&lt;/li&gt;&#xA;&lt;li&gt;互信息和最大信息系数&lt;/li&gt;&#xA;&lt;li&gt;距离相关系数&lt;/li&gt;&#xA;&lt;li&gt;方差选择法&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;12-决策树的剪枝&#34;&gt;1.2 决策树的剪枝&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;过拟合&#xA;&lt;ul&gt;&#xA;&lt;li&gt;学习器能力过于强大，把训练样本自身的一些特点当做了所有潜在样本都会具有的一般性质，导致泛化性能下降&lt;/li&gt;&#xA;&lt;li&gt;无法彻底避免，只能做到“缓解”&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;欠拟合&#xA;&lt;ul&gt;&#xA;&lt;li&gt;学习器能力低下，对训练样本的一般性质尚未学好&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;剪枝的目的：通过主动去掉一些分支来降低过拟合的风险&lt;/li&gt;&#xA;&lt;li&gt;策略&#xA;&lt;ul&gt;&#xA;&lt;li&gt;预剪枝&#xA;&lt;ul&gt;&#xA;&lt;li&gt;生成过程中的剪枝，在每个节点选择完之后进行一次验证&lt;/li&gt;&#xA;&lt;li&gt;评估精度：正确分类的样本占所有样本的比例&lt;/li&gt;&#xA;&lt;li&gt;不足：基于“贪心”本质禁止某些分支展开，带来了欠拟合的风险&lt;/li&gt;&#xA;&lt;li&gt;优点：降低过拟合的风险，减少了训练时间开销和测试时间开销&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;后剪枝&#xA;&lt;ul&gt;&#xA;&lt;li&gt;先从训练集生成一棵完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树替换为叶节点能带来决策树泛化性能提升(通过准确率来判断)，则将该子树替换为叶节点&lt;/li&gt;&#xA;&lt;li&gt;优点：保留了更多的分支；欠拟合风险很小；泛化能力优于预剪枝&lt;/li&gt;&#xA;&lt;li&gt;缺点：消耗时间多&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;13-回归决策树&#34;&gt;1.3 回归决策树&lt;/h2&gt;&#xA;&lt;h3 id=&#34;回归树&#34;&gt;回归树&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;连续属性离散化技术&#xA;&lt;ul&gt;&#xA;&lt;li&gt;二分法 C4.5决策树算法&lt;/li&gt;&#xA;&lt;li&gt;选择划分点，像离散属性值一样来考察这些划分点，选取最优的划分点进行样本集合的划分&lt;/li&gt;&#xA;&lt;li&gt;与离散属性不同，若当前节点划分属性为连续属性，该连续属性还可被再次选作后代节点的最优划分属性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;14-属性缺失&#34;&gt;1.4 属性缺失&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;属性值缺失时，如何进行划分属性选择？如何计算信息增益&#xA;&lt;ul&gt;&#xA;&lt;li&gt;不考虑缺失值，乘以无缺失值样本所占比例&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;如何对样本进行划分&#xA;&lt;ul&gt;&#xA;&lt;li&gt;属性值未知时，划入所有子节点，并调整权制&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;15-多变量决策树&#34;&gt;1.5 多变量决策树&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;属性的线性组合(线性分类器)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;16-总结&#34;&gt;1.6 总结&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;是一种基于规则的方法，嵌套规则进行预测，根据判断结果进入一个分支，反复执行这个操作直到叶子节点，得到预测结果。这些规则通过训练得到，而非人工制定。&lt;/li&gt;&#xA;&lt;li&gt;本质上决策树是通过一系列规则对数据进行分类的过程。首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。&lt;/li&gt;&#xA;&lt;li&gt;决策树的优点&#xA;&lt;ul&gt;&#xA;&lt;li&gt;推理过程容易理解，决策推理过程可以表示成If Then 形式；&lt;/li&gt;&#xA;&lt;li&gt;推理过程完全依赖于属性变量的取值特点；&lt;/li&gt;&#xA;&lt;li&gt;可自动忽略目标变量没有贡献的属性变量，也为判断属性变量的重要性，减少变量的数目提供参考。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;2-贝叶斯模型&#34;&gt;2. 贝叶斯模型&lt;/h1&gt;&#xA;&lt;h2 id=&#34;21-贝叶斯决策论&#34;&gt;2.1 贝叶斯决策论&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;条件概率&lt;/li&gt;&#xA;&lt;li&gt;分类问题&lt;/li&gt;&#xA;&lt;li&gt;分类器&lt;/li&gt;&#xA;&lt;li&gt;机器学习所要实现的是基于有限的训练样本尽可能准确地估计出后验概率&lt;/li&gt;&#xA;&lt;li&gt;两类基本策略&#xA;&lt;ul&gt;&#xA;&lt;li&gt;判别式模型&#xA;&lt;ul&gt;&#xA;&lt;li&gt;直接建模P(c | x), 决策树，BP神经网络、SVM&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;生成式模型&#xA;&lt;ul&gt;&#xA;&lt;li&gt;先建模联合概率分布P(x, c)，再计算P(c | x)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;P(c | x) = P(x, c) / P(x) -&amp;gt; P(c | x) = P(x, c)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;贝叶斯分类器&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;本质区别&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;22-朴素贝叶斯分类器nb&#34;&gt;2.2 朴素贝叶斯分类器(NB)&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;假设属性之间条件独立&#xA;&lt;ul&gt;&#xA;&lt;li&gt;不必计算X和Y的每一种组合的类条件概率，只需对给定的Y，计算每个x&lt;!-- raw HTML omitted --&gt;i&lt;!-- raw HTML omitted --&gt;的条件概率，不需要很大的训练集就能获得较好的概率估计&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;计算联合概率，具有较大的联合概率的x即为分类结果&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基于训练器D来估计类先验概率P(c)，并为每个属性估计条件概率P(x&lt;!-- raw HTML omitted --&gt;i&lt;!-- raw HTML omitted --&gt; | c)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;对于连续变量&#xA;&lt;ul&gt;&#xA;&lt;li&gt;使用正态分布曲线来拟合-&amp;gt;估算新的值的概率&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;拉普拉斯修正&#xA;&lt;ul&gt;&#xA;&lt;li&gt;某个属性值在训练集中没有与某个类同时出现过，则直接计算连乘式的概率值为0，导致分类结果显然不合理&lt;/li&gt;&#xA;&lt;li&gt;为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”，在估计概率值时通常要进行“拉普拉斯修正”&lt;/li&gt;&#xA;&lt;li&gt;P(c) = |D&lt;!-- raw HTML omitted --&gt;c&lt;!-- raw HTML omitted --&gt; + 1| / |D| + N&lt;/li&gt;&#xA;&lt;li&gt;若任务对预测速率要求较高：估值存储&lt;/li&gt;&#xA;&lt;li&gt;若任务数据更替频繁：懒惰学习，先不进行任务训练，收到预测请求时再估值&lt;/li&gt;&#xA;&lt;li&gt;脱任务数据不断增加：增量学习，基于现有估值，对新样本涉及的概率估值进行修正&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;23-半朴素贝叶斯分类器&#34;&gt;2.3 半朴素贝叶斯分类器&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;独依赖估计 ODE&#xA;&lt;ul&gt;&#xA;&lt;li&gt;每个属性再类别之外最多依赖一个其他属性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;SPODE&#xA;&lt;ul&gt;&#xA;&lt;li&gt;假设所有属性都依赖于同一属性，称为“超父”&lt;/li&gt;&#xA;&lt;li&gt;通过交叉验证等模型选择方法来确定超父属性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;TAN&#xA;&lt;ul&gt;&#xA;&lt;li&gt;以属性间的条件“互信息”为边构建网络属性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;24-贝叶斯网络&#34;&gt;2.4 贝叶斯网络&lt;/h2&gt;</description>
    </item>
  </channel>
</rss>
